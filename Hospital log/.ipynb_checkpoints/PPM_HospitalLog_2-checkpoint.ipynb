{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10f68cb7",
   "metadata": {},
   "source": [
    "## Hospital Event Logs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4115026d",
   "metadata": {},
   "source": [
    "- <b>Processo</b>: Os dados retratam um processo de tratamento hospitalar que começa com o registro em um pronto-socorro ou departamento de família e avança pelas etapas de exame, diagnóstico e tratamento. Nomeadamente, os tratamentos mal sucedidos implicam frequentemente ciclos repetitivos de diagnóstico e tratamento, sublinhando a natureza iterativa da prestação de cuidados de saúde.\n",
    "\n",
    "- <b> Atributos</b>: Os registros incorporam atributos do paciente, como idade, condição subjacente, cidadania, proficiência na língua alemã, sexo e seguro privado. Esses atributos, influenciando o processo de tratamento, podem revelar potencial discriminação. Fatores como a idade e a condição podem afetar a complexidade dos casos e o percurso do tratamento, enquanto a cidadania pode realçar disparidades no acesso aos cuidados de saúde. A proficiência em alemão pode impactar a comunicação prestador-paciente, afetando assim a qualidade do atendimento. O género poderia destacar potenciais disparidades na saúde, enquanto o estatuto do seguro poderia indicar influências socioeconómicas na qualidade ou oportunidade dos cuidados. Portanto, uma análise abrangente destes atributos relativamente ao processo de tratamento poderia lançar luz sobre potenciais preconceitos ou disparidades, promovendo a justiça na prestação de cuidados de saúde.\n",
    "\n",
    "- <b>Três registros de eventos</b>: os três registos de eventos representam vários graus de discriminação, oferecendo aos investigadores uma oportunidade de explorar as nuances e complexidades que surgem em diversos cenários do mundo real."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00a592d",
   "metadata": {},
   "source": [
    "| Base | Eventos | Cases | Variantes | Atividades |\n",
    "| --- | --- | --- | --- | --- |\n",
    "| hospital_log_high | 69528 | 10000 | 80 | 10 |\n",
    "| hospital_log_low | 70037 | 10000 | 106 | 10 |\n",
    "| hospital_log_medim | 70124 | 10000 | 77 | 10 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32debc2",
   "metadata": {},
   "source": [
    "- <b>Atributos sensíveis</b>:\n",
    "    - Age\n",
    "    - Citizenship\n",
    "    - German Proficiency\n",
    "    - Gender\n",
    "    - Condição subjacente\n",
    "    - Seguro privado- <b>Atributos sensíveis</b>:\n",
    "    - Age\n",
    "    - Citizenship\n",
    "    - German Proficiency\n",
    "    - Gender\n",
    "    - Religion\n",
    "    - Years of Education"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75569df8",
   "metadata": {},
   "source": [
    "### Importar bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "811138aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micka\\anaconda3\\Lib\\site-packages\\torch\\_functorch\\deprecated.py:61: UserWarning: We've integrated functorch into PyTorch. As the final step of the integration, functorch.vmap is deprecated as of PyTorch 2.0 and will be deleted in a future version of PyTorch >= 2.3. Please use torch.vmap instead; see the PyTorch 2.0 release notes and/or the torch.func migration guide for more details https://pytorch.org/docs/master/func.migrating.html\n",
      "  warn_deprecated('vmap', 'torch.vmap')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\micka\\OneDrive\\Documentos\\[EACH USP] Doutorado\\2. Pesquisa\\6. Experimento\\3. Predictive Process Monitoring\\_PythonProcessMining\\AIF360.py:339: The name tf.disable_v2_behavior is deprecated. Please use tf.compat.v1.disable_v2_behavior instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\micka\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\compat\\v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "WARNING:tensorflow:From C:\\Users\\micka\\AppData\\Local\\Temp\\ipykernel_15872\\1331545630.py:24: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pm4py\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "#Biblioteca implementada com funções customizadas\n",
    "# Adicionar o caminho da pasta onde está a biblioteca ao sys.path\n",
    "sys.path.append(os.path.abspath('C:\\\\Users\\\\micka\\\\OneDrive\\\\Documentos\\\\[EACH USP] Doutorado\\\\2. Pesquisa\\\\6. Experimento\\\\3. Predictive Process Monitoring'))\n",
    "from _PythonProcessMining import TraceEncoding\n",
    "from _PythonProcessMining import ResourceEncoding\n",
    "from _PythonProcessMining import DataPrep\n",
    "from _PythonProcessMining import TrainTestSplit\n",
    "from _PythonProcessMining import MachineLearning\n",
    "from _PythonProcessMining import Metrics\n",
    "from _PythonProcessMining import MachineLearninExplanation\n",
    "from _PythonProcessMining import AIF360\n",
    "\n",
    "# Definir seeds para reprodutibilidade\n",
    "tf.set_random_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1e4953",
   "metadata": {},
   "source": [
    "### Importar a base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e3053c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "diretorio_log = \"C:\\\\Users\\\\micka\\\\OneDrive\\\\Documentos\\\\[EACH USP] Doutorado\\\\2. Pesquisa\\\\6. Experimento\\\\0. Logs\\\\Hospital log\\\\\"\n",
    "diretorio = \".\\\\Hospital log\\\\\"\n",
    "name_prefix = 'hospital_log'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b930eb3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66dcb356aec44bf0be5074227d5e7ed1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "parsing log, completed traces ::   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c11aa4874b3422d9559b9cd66e9edd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "parsing log, completed traces ::   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efd3ca67c20945d6805b78563fe3eafb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "parsing log, completed traces ::   0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho da base de dados - High: (69528, 16)\n",
      "Tamanho da base de dados - Medium: (70124, 16)\n",
      "Tamanho da base de dados - Low: (70037, 16)\n"
     ]
    }
   ],
   "source": [
    "log_high = pm4py.read_xes(os.path.join(diretorio_log, f'{name_prefix}_high-xes.gz'))\n",
    "log_medium = pm4py.read_xes(os.path.join(diretorio_log, f'{name_prefix}_medium-xes.gz'))\n",
    "log_low = pm4py.read_xes(os.path.join(diretorio_log, f'{name_prefix}_low-xes.gz'))\n",
    "\n",
    "print(\"Tamanho da base de dados - High:\", log_high.shape)\n",
    "print(\"Tamanho da base de dados - Medium:\", log_medium.shape)\n",
    "print(\"Tamanho da base de dados - Low:\", log_low.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72ba666f",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_high.to_csv(f'{name_prefix}_high-csv.csv')\n",
    "log_medium.to_csv(f'{name_prefix}_medium-csv.csv')\n",
    "log_low.to_csv(f'{name_prefix}_low-csv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffc2f61b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho da base de dados - High: (69528, 16)\n",
      "Tamanho da base de dados - Medium: (70124, 16)\n",
      "Tamanho da base de dados - Low: (70037, 16)\n"
     ]
    }
   ],
   "source": [
    "print(\"Tamanho da base de dados - High:\", log_high.shape)\n",
    "print(\"Tamanho da base de dados - Medium:\", log_medium.shape)\n",
    "print(\"Tamanho da base de dados - Low:\", log_low.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e495b9",
   "metadata": {},
   "source": [
    "### Preparação da base"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916954c6",
   "metadata": {},
   "source": [
    "<b>Resultado do case</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7609973f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def def_determine_result(df):\n",
    "    \"\"\"\n",
    "    Marca o resultado de cada case com base nas atividades 'Treatment unsuccessful' e 'Treatment successful',\n",
    "    e EXCLUI todas as atividades que ocorrem após a última dessas atividades.\n",
    "    O resultado é definido pela última ocorrência dessas atividades ao iterar de trás para frente:\n",
    "    - 'Treatment unsuccessful' -> Resultado negativo.\n",
    "    - 'Treatment successful' -> Resultado positivo.\n",
    "    - Nenhuma das atividades -> Resultado negativo. --- Se o resultado é indefinido, marcamos como item negativo.\n",
    "\n",
    "    Parâmetros:\n",
    "    - df (pd.DataFrame): DataFrame contendo os dados do log de eventos.\n",
    "\n",
    "    Retorna:\n",
    "    - pd.DataFrame: DataFrame processado e filtrado.\n",
    "    \"\"\"\n",
    "\n",
    "    # Inicializar o dicionário para armazenar os resultados e índices de corte\n",
    "    resultados = {}\n",
    "    filtered_records = []\n",
    "\n",
    "    # Ordenar o DataFrame pelo ID do caso e pelo timestamp\n",
    "    df_sorted = df.sort_values(by=['case:concept:name', 'time:timestamp'])\n",
    "\n",
    "    # Iterar sobre cada caso\n",
    "    for case_id, grupo in df_sorted.groupby('case:concept:name'):\n",
    "        # Inicializar o resultado como indefinido e o índice de corte\n",
    "        resultado = 'Negative'\n",
    "        last_occurrence_index = None\n",
    "\n",
    "        # Iterar de trás para frente para encontrar a última ocorrência relevante\n",
    "        for idx, row in grupo[::-1].iterrows():\n",
    "            if row['concept:name'] == 'Treatment unsuccessful':\n",
    "                resultado = 'Negative'\n",
    "                last_occurrence_index = idx\n",
    "                break\n",
    "            elif row['concept:name'] == 'Treatment successful':\n",
    "                resultado = 'Positive'\n",
    "                last_occurrence_index = idx\n",
    "                break\n",
    "\n",
    "        # Marcar o resultado do case\n",
    "        resultados[case_id] = resultado\n",
    "\n",
    "        # Filtrar o grupo para incluir apenas as atividades até a última ocorrência identificada\n",
    "        if last_occurrence_index is not None:\n",
    "            grupo_filtrado = grupo.loc[:last_occurrence_index]\n",
    "        else:\n",
    "            grupo_filtrado = grupo  # Caso nenhuma atividade de interesse tenha sido encontrada, manter o grupo completo\n",
    "\n",
    "        # Adicionar os registros filtrados à lista\n",
    "        filtered_records.append(grupo_filtrado)\n",
    "\n",
    "    # Combinar todos os grupos filtrados em um único DataFrame\n",
    "    filtered_df = pd.concat(filtered_records)\n",
    "\n",
    "    # Adicionar a coluna de resultados ao DataFrame filtrado\n",
    "    filtered_df['Resultado'] = filtered_df['case:concept:name'].map(resultados)\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "746d727c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log_high = def_determine_result(log_high)\n",
    "df_log_medium = def_determine_result(log_medium)\n",
    "df_log_low = def_determine_result(log_low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "90653d8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho da base de dados - High: (60127, 17)\n",
      "Tamanho da base de dados - Medium: (60363, 17)\n",
      "Tamanho da base de dados - Low: (60131, 17)\n"
     ]
    }
   ],
   "source": [
    "print(\"Tamanho da base de dados - High:\", df_log_high.shape)\n",
    "print(\"Tamanho da base de dados - Medium:\", df_log_medium.shape)\n",
    "print(\"Tamanho da base de dados - Low:\", df_log_low.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "759e0000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High: Resultado\n",
      "Positive    54919\n",
      "Negative     5208\n",
      "Name: count, dtype: int64\n",
      "Medium: Resultado\n",
      "Positive    57888\n",
      "Negative     2475\n",
      "Name: count, dtype: int64\n",
      "Low: Resultado\n",
      "Positive    59368\n",
      "Negative      763\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('High:', df_log_high['Resultado'].value_counts())\n",
    "print('Medium:', df_log_medium['Resultado'].value_counts())\n",
    "print('Low:', df_log_low['Resultado'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0f6b99b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "High: Resultado\n",
      "Positive    54919\n",
      "Negative     5208\n",
      "Name: count, dtype: int64\n",
      "Medium: Resultado\n",
      "Positive    57888\n",
      "Negative     2475\n",
      "Name: count, dtype: int64\n",
      "Low: Resultado\n",
      "Positive    59368\n",
      "Negative      763\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('High:', df_log_high['Resultado'].value_counts())\n",
    "print('Medium:', df_log_medium['Resultado'].value_counts())\n",
    "print('Low:', df_log_low['Resultado'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0fa56de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def def_descriptive_results(df):\n",
    "    # Parte 1: Eliminar duplicidade de caso\n",
    "    df_unico = df.drop_duplicates(subset='case:concept:name', keep='first')\n",
    "    \n",
    "    # Parte 2: Calcular o % de registros com 'Resultado' igual a 1\n",
    "    total_registros = len(df_unico)  # Total de registros após eliminar duplicidades\n",
    "    registros_resultado = len(df_unico[df_unico['Resultado'] == 'Positive'])  # Registros com Resultado == 1\n",
    "    percentual_resultado = (registros_resultado / total_registros) * 100  # Cálculo do percentual\n",
    "    \n",
    "    return percentual_resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "20dafa0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentual de registros com 'Resultado' positivo - High: 90.05%\n",
      "Percentual de registros com 'Resultado' positivo - Medium: 95.36%\n",
      "Percentual de registros com 'Resultado' positivo - Low: 98.35%\n"
     ]
    }
   ],
   "source": [
    "print(\"Percentual de registros com 'Resultado' positivo - High: {:.2f}%\".format(def_descriptive_results(df_log_high)))\n",
    "print(\"Percentual de registros com 'Resultado' positivo - Medium: {:.2f}%\".format(def_descriptive_results(df_log_medium)))\n",
    "print(\"Percentual de registros com 'Resultado' positivo - Low: {:.2f}%\".format(def_descriptive_results(df_log_low)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ac1d4965",
   "metadata": {},
   "outputs": [],
   "source": [
    "def def_descriptive_protected(df):\n",
    "    # Parte 1: Eliminar duplicidade de caso\n",
    "    df_unico = df.drop_duplicates(subset='case:concept:name', keep='first')\n",
    "    \n",
    "    # Parte 2: Calcular o % de registros com 'Resultado' igual a 1\n",
    "    total_registros = len(df_unico)  # Total de registros após eliminar duplicidades\n",
    "    registros_resultado = len(df_unico[df_unico['protected'] == True])  # Registros com Resultado == 1\n",
    "    percentual_resultado = (registros_resultado / total_registros) * 100  # Cálculo do percentual\n",
    "    \n",
    "    return percentual_resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1368be63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentual de registros com 'Atribuito Protected' positivo - High: 30.39%\n",
      "Percentual de registros com 'Atribuito Protected' positivo - Medium: 21.28%\n",
      "Percentual de registros com 'Atribuito Protected' positivo - Low: 10.49%\n"
     ]
    }
   ],
   "source": [
    "print(\"Percentual de registros com 'Atribuito Protected' positivo - High: {:.2f}%\".format(def_descriptive_protected(df_log_high)))\n",
    "print(\"Percentual de registros com 'Atribuito Protected' positivo - Medium: {:.2f}%\".format(def_descriptive_protected(df_log_medium)))\n",
    "print(\"Percentual de registros com 'Atribuito Protected' positivo - Low: {:.2f}%\".format(def_descriptive_protected(df_log_low)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e0d904",
   "metadata": {},
   "source": [
    "<b> Variável Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "350b5e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar a nova coluna 'Target' mapeando 'Negativo' para 0 e 'Positivo' para 1\n",
    "df_log_high['Target'] = df_log_high['Resultado'].replace({'Negative': 0, 'Positive': 1})\n",
    "df_log_medium['Target'] = df_log_medium['Resultado'].replace({'Negative': 0, 'Positive': 1})\n",
    "df_log_low['Target'] = df_log_low['Resultado'].replace({'Negative': 0, 'Positive': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e27a760",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tamanho da base de dados - High: (60127, 18)\n",
      "Tamanho da base de dados - Medium: (60363, 18)\n",
      "Tamanho da base de dados - Low: (60131, 18)\n"
     ]
    }
   ],
   "source": [
    "print(\"Tamanho da base de dados - High:\", df_log_high.shape)\n",
    "print(\"Tamanho da base de dados - Medium:\", df_log_medium.shape)\n",
    "print(\"Tamanho da base de dados - Low:\", df_log_low.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0fbdcec3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>High</th>\n",
       "      <th>Medium</th>\n",
       "      <th>Low</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>% Protected = 1</th>\n",
       "      <td>30.39%</td>\n",
       "      <td>21.28%</td>\n",
       "      <td>10.49%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>% Protected = 1 &amp; Target = 1</th>\n",
       "      <td>20.44%</td>\n",
       "      <td>16.64%</td>\n",
       "      <td>8.84%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Demographic Parity</th>\n",
       "      <td>0.33</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Disparate Impact</th>\n",
       "      <td>0.67</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total</th>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Target = 0</th>\n",
       "      <td>995</td>\n",
       "      <td>464</td>\n",
       "      <td>165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Total Target = 1</th>\n",
       "      <td>9005</td>\n",
       "      <td>9536</td>\n",
       "      <td>9835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                High  Medium     Low\n",
       "% Protected = 1               30.39%  21.28%  10.49%\n",
       "% Protected = 1 & Target = 1  20.44%  16.64%   8.84%\n",
       "Demographic Parity              0.33    0.22    0.16\n",
       "Disparate Impact                0.67    0.78    0.84\n",
       "Total                          10000   10000   10000\n",
       "Total Target = 0                 995     464     165\n",
       "Total Target = 1                9005    9536    9835"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protected_attribute = 'protected'\n",
    "outcome_attribute = 'Target'\n",
    "privileged_group = 0\n",
    "unprivileged_group = 1\n",
    "\n",
    "\n",
    "# Aplicar a função de resumo para cada base\n",
    "summary_high = TrainTestSplit.Descriptive(df_log_high, \"High\", outcome_attribute, protected_attribute, privileged_group, unprivileged_group)\n",
    "summary_medium = TrainTestSplit.Descriptive(df_log_medium, \"Medium\", outcome_attribute, protected_attribute, privileged_group, unprivileged_group)\n",
    "summary_low = TrainTestSplit.Descriptive(df_log_low, \"Low\", outcome_attribute, protected_attribute, privileged_group, unprivileged_group)\n",
    "\n",
    "# Combinar todas as tabelas de resumo em uma única tabela\n",
    "df_summary_table = pd.concat([summary_high, summary_medium, summary_low], axis=1)\n",
    "df_summary_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10bbb36",
   "metadata": {},
   "source": [
    "<b> Data de início e data de fim do case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f65abf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log_high = df_log_high.copy()\n",
    "df_log_medium = df_log_medium.copy()\n",
    "df_log_low = df_log_low.copy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "582fe03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def def_TimestampStartEnd(df):\n",
    "    # Converter 'time:timestamp' para datetime\n",
    "    df['time:timestamp'] = pd.to_datetime(df['time:timestamp'])\n",
    "\n",
    "    # Calcular a data de início do case\n",
    "    df['case_start_date'] = df.groupby('case:concept:name')['time:timestamp'].transform('min')\n",
    "\n",
    "    # Calcular a data de fim do case\n",
    "    df['case_end_date'] = df.groupby('case:concept:name')['time:timestamp'].transform('max')\n",
    "\n",
    "    # Calcular o lead time em dias\n",
    "    df['lead_time_days'] = (df['case_end_date'] - df['case_start_date']).dt.days\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756a8b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicionar datas de início e fim do case\n",
    "df_log_high = def_TimestampStartEnd(df_log_high)\n",
    "df_log_medium = def_TimestampStartEnd(df_log_medium)\n",
    "df_log_low = def_TimestampStartEnd(df_log_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8152b2ac",
   "metadata": {},
   "source": [
    "<b>Atributos numéricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a90b24ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Idade\n",
    "    #Infância (0-12 anos)\n",
    "    #Adolescência (13-17 anos)\n",
    "    #Juventude (18-24 anos)\n",
    "    #Adulto jovem (25-34 anos)\n",
    "    #Adulto de meia-idade (35-49 anos)\n",
    "    #Pré-aposentadoria (50-64 anos)\n",
    "    #Aposentado (65 anos ou mais)\n",
    "    \n",
    "# Categorizar idade\n",
    "bins = [0, 12, 17, 24, 34, 49, 64, float('inf')]\n",
    "labels = ['Childhood', 'Adolescence', 'Youth', 'Young Adult', 'Middle-Aged Adult', 'Pre-Retirement', 'Retired']\n",
    "\n",
    "#Aplicar as bases de dados\n",
    "df_log_high['age'] = pd.cut(df_log_high['case:age'], bins=bins, labels=labels, right=False)\n",
    "df_log_medium['age'] = pd.cut(df_log_medium['case:age'], bins=bins, labels=labels, right=False)\n",
    "df_log_low['age'] = pd.cut(df_log_low['case:age'], bins=bins, labels=labels, right=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4378af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gerar dummies das categorizações\n",
    "df_log_high = pd.get_dummies(df_log_high, columns=['age'])\n",
    "df_log_medium = pd.get_dummies(df_log_medium, columns=['age'])\n",
    "df_log_low = pd.get_dummies(df_log_low, columns=['age'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ab3da4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertendo valores booleanos true/false para 1/0\n",
    "bool_columns = ['age_Childhood', 'age_Adolescence', 'age_Youth', 'age_Young Adult', 'age_Middle-Aged Adult', 'age_Pre-Retirement', 'age_Retired',\n",
    "               ]\n",
    "\n",
    "df_log_high[bool_columns] = df_log_high[bool_columns].astype(int)\n",
    "df_log_medium[bool_columns] = df_log_medium[bool_columns].astype(int)\n",
    "df_log_low[bool_columns] = df_log_low[bool_columns].astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71acffe8",
   "metadata": {},
   "source": [
    "<b>Atributos categóricos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86997e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Renomear atributo protegido\n",
    "df_log_high = df_log_high.rename(columns={'protected': 'case:protected'})\n",
    "df_log_medium = df_log_medium.rename(columns={'protected': 'case:protected'})\n",
    "df_log_low = df_log_low.rename(columns={'protected': 'case:protected'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd5d2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertendo valores booleanos true/false para 1/0\n",
    "atributos_categoricos = ['case:protected', 'case:german speaking', 'case:gender', 'case:citizen', 'case:private_insurance', 'case:underlying_condition']\n",
    "\n",
    "df_log_high[atributos_categoricos] = df_log_high[atributos_categoricos].astype(int)\n",
    "df_log_medium[atributos_categoricos] = df_log_medium[atributos_categoricos].astype(int)\n",
    "df_log_low[atributos_categoricos] = df_log_low[atributos_categoricos].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73440b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extrair base tratada\n",
    "df_log_high.to_csv(f'{name_prefix}_high-prep.csv')\n",
    "df_log_medium.to_csv(f'{name_prefix}_medium-prep.csv')\n",
    "df_log_low.to_csv(f'{name_prefix}_low-prep.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1e77ac",
   "metadata": {},
   "source": [
    "<b> Trace encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed267c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_high_transitions, log_high_all_transitions = TraceEncoding.ActivityTransitionsOneHotEncoding(df_log_high, var_id = 'case:case', var_activity = 'activity', var_timestamp = 'time:timestamp')\n",
    "log_medium_transitions, log_medium_all_transitions = TraceEncoding.ActivityTransitionsOneHotEncoding(df_log_medium, var_id = 'case:case', var_activity = 'activity', var_timestamp = 'time:timestamp')\n",
    "log_low_transitions, log_low_all_transitions = TraceEncoding.ActivityTransitionsOneHotEncoding(df_log_low, var_id = 'case:case', var_activity = 'activity', var_timestamp = 'time:timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29ca51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trazer o encoding das transições para o dataframe\n",
    "df_log_high = df_log_high.merge(log_high_transitions, on = 'case:case')\n",
    "df_log_medium = df_log_medium.merge(log_medium_transitions, on = 'case:case')\n",
    "df_log_low = df_log_low.merge(log_low_transitions, on = 'case:case')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fec2ba",
   "metadata": {},
   "source": [
    "<b> Resource encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204a1db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_high_resources, log_high_all_resources = ResourceEncoding.ResourceTransitionsOneHotEncoding(df_log_high, var_id = 'case:case', var_resource = 'resource', var_timestamp = 'time:timestamp')\n",
    "log_medium_resources, log_medium_all_resources = ResourceEncoding.ResourceTransitionsOneHotEncoding(df_log_medium, var_id = 'case:case', var_resource = 'resource', var_timestamp = 'time:timestamp')\n",
    "log_low_resources, log_low_all_resources = ResourceEncoding.ResourceTransitionsOneHotEncoding(df_log_low, var_id = 'case:case', var_resource = 'resource', var_timestamp = 'time:timestamp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d7a19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trazer o encoding das transições para o dataframe\n",
    "df_log_high = df_log_high.merge(log_high_resources, on = 'case:case')\n",
    "df_log_medium = df_log_medium.merge(log_medium_resources, on = 'case:case')\n",
    "df_log_low = df_log_low.merge(log_low_resources, on = 'case:case')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497aa3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mater apenas um registro por case\n",
    "df_log_high = df_log_high.drop_duplicates(subset=['case:case']).drop(columns=['activity', 'resource', 'time', 'concept:name', 'time:timestamp', '@@index', 'case:concept:name', 'case:@@case_index'])\n",
    "df_log_medium = df_log_medium.drop_duplicates(subset=['case:case']).drop(columns=['activity', 'resource', 'time', 'concept:name', 'time:timestamp', '@@index', 'case:concept:name', 'case:@@case_index'])\n",
    "df_log_low = df_log_low.drop_duplicates(subset=['case:case']).drop(columns=['activity', 'resource', 'time', 'concept:name', 'time:timestamp', '@@index', 'case:concept:name', 'case:@@case_index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9183c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extrair base tratada\n",
    "log_high.to_csv(f'{name_prefix}_high-tratada.csv')\n",
    "log_medium.to_csv(f'{name_prefix}_medium-tratada.csv')\n",
    "log_low.to_csv(f'{name_prefix}_low-tratada.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d940c8c",
   "metadata": {},
   "source": [
    "<b>Eliminar Duplicidades/conflitos de casos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c246ee30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lista de variáveis que NÃO serão consideradas no modelo\n",
    "exclude_variavel = [\n",
    "             'case:case', \n",
    "             'case_start_date', \n",
    "             'case_end_date', \n",
    "             'lead_time_days', \n",
    "             'Resultado',\n",
    "             #'Target', \n",
    "             #'case:protected',\n",
    "             #'case:german speaking', \n",
    "             #'case:gender', \n",
    "             #'case:citizen', \n",
    "             #'case:private_insurance', \n",
    "             #'case:underlying_condition',\n",
    "             'case:age', \n",
    "             'age_Childhood',\n",
    "             'age_Adolescence', \n",
    "             'age_Youth', \n",
    "             #'age_Young Adult',\n",
    "             #'age_Middle-Aged Adult', \n",
    "             #'age_Pre-Retirement', \n",
    "             #'age_Retired'\n",
    "]\n",
    "\n",
    "#Criar a lista 'variaveis', excluindo as atividades que contenha o resultado do case\n",
    "todas_variaveis = set(df_log_high.columns) | set(df_log_medium.columns) | set(df_log_low.columns)\n",
    "variaveis = [\n",
    "    col for col in todas_variaveis  \n",
    "    if col not in exclude_variavel\n",
    "]\n",
    "\n",
    "print(variaveis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ffbfcb",
   "metadata": {},
   "source": [
    "Identificar e eliminar registros conflitantes na base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6e8d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conflicting_groups_high, conflicting_cases_high, df_log_high_cleaned = DataPrep.KeepFirstConflictingCases(df_log_high, variaveis, var_timestamp = 'time:timestamp', var_target = 'Target', var_id = 'case:case', var_date = 'case_start_date')\n",
    "conflicting_groups_medium, conflicting_cases_medium, df_log_medium_cleaned = DataPrep.KeepFirstConflictingCases(df_log_medium, variaveis, var_timestamp = 'time:timestamp', var_target = 'Target', var_id = 'case:case', var_date = 'case_start_date')\n",
    "conflicting_groups_low, conflicting_cases_low, df_log_low_cleaned = DataPrep.KeepFirstConflictingCases(df_log_low, variaveis, var_timestamp = 'time:timestamp', var_target = 'Target', var_id = 'case:case', var_date = 'case_start_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84833549",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log_high = df_log_high_cleaned.copy()\n",
    "df_log_medium = df_log_medium_cleaned.copy()\n",
    "df_log_low = df_log_low_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e42af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tamanho da base de dados - High:\", df_log_high.shape)\n",
    "print(\"Tamanho da base de dados - Medium:\", df_log_medium.shape)\n",
    "print(\"Tamanho da base de dados - Low:\", df_log_low.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1913963b",
   "metadata": {},
   "source": [
    "Identificar e eliminar duplicidades de registros na base de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80544e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encontrar casos duplicados\n",
    "df_log_high_duplicates, df_log_high_merged, df_log_high_cleaned = DataPrep.KeepFirstDuplicateCases(df_log_high, variaveis, var_timestamp = 'time:timestamp', var_target = 'Target', var_id = 'case:case', var_date = 'case_start_date')\n",
    "df_log_medium_duplicates, df_log_medium_merged, df_log_medium_cleaned = DataPrep.KeepFirstDuplicateCases(df_log_medium, variaveis, var_timestamp = 'time:timestamp', var_target = 'Target', var_id = 'case:case', var_date = 'case_start_date')\n",
    "df_log_low_duplicates, df_log_low_merged, df_log_low_cleaned = DataPrep.KeepFirstDuplicateCases(df_log_low, variaveis, var_timestamp = 'time:timestamp', var_target = 'Target', var_id = 'case:case', var_date = 'case_start_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066aaf71",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_log_high = df_log_high_cleaned.copy()\n",
    "df_log_medium = df_log_medium_cleaned.copy()\n",
    "df_log_low = df_log_low_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d83d8415",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Tamanho da base de dados - High:\", df_log_high.shape)\n",
    "print(\"Tamanho da base de dados - Medium:\", df_log_medium.shape)\n",
    "print(\"Tamanho da base de dados - Low:\", df_log_low.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbe06d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extrair base tratada\n",
    "df_log_high.to_csv(f'{name_prefix}_high-tratada-semdpl.csv')\n",
    "df_log_medium.to_csv(f'{name_prefix}_medium-tratada-semdpl.csv')\n",
    "df_log_low.to_csv(f'{name_prefix}_low-tratada-semdpl.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cf8579",
   "metadata": {},
   "outputs": [],
   "source": [
    "protected_attribute = 'case:protected'\n",
    "outcome_attribute = 'Target'\n",
    "privileged_group = 0\n",
    "unprivileged_group = 1\n",
    "\n",
    "\n",
    "# Aplicar a função de resumo para cada base\n",
    "summary_high = TrainTestSplit.Descriptive(df_log_high, \"High\", outcome_attribute, protected_attribute, privileged_group, unprivileged_group)\n",
    "summary_medium = TrainTestSplit.Descriptive(df_log_medium, \"Medium\", outcome_attribute, protected_attribute, privileged_group, unprivileged_group)\n",
    "summary_low = TrainTestSplit.Descriptive(df_log_low, \"Low\", outcome_attribute, protected_attribute, privileged_group, unprivileged_group)\n",
    "\n",
    "# Combinar todas as tabelas de resumo em uma única tabela\n",
    "df_summary_table = pd.concat([summary_high, summary_medium, summary_low], axis=1)\n",
    "df_summary_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309d00be",
   "metadata": {},
   "source": [
    "<b> Divisão da base em treino/teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6720d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar a divisão entre treino e teste\n",
    "  #Pela data de início do case\n",
    "df_train_high, df_test_high = TrainTestSplit.SplitDataTemporal(df_log_high, test_size=0.3, var_date = 'case_start_date')\n",
    "df_train_medium, df_test_medium = TrainTestSplit.SplitDataTemporal(df_log_medium, test_size=0.3, var_date = 'case_start_date')\n",
    "df_train_low, df_test_low = TrainTestSplit.SplitDataTemporal(df_log_low, test_size=0.3, var_date = 'case_start_date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24d61de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copia dos dataframe para resolver problema de fragmentação\n",
    "df_train_high = df_train_high.copy()\n",
    "df_test_high = df_test_high.copy()\n",
    "df_train_medium = df_train_medium.copy()\n",
    "df_test_medium = df_test_medium.copy()\n",
    "df_train_low = df_train_low.copy()\n",
    "df_test_low = df_test_low.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09368f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "protected_attribute = 'case:protected'\n",
    "outcome_attribute = 'Target'\n",
    "privileged_group = 0\n",
    "unprivileged_group = 1\n",
    "\n",
    "\n",
    "# Aplicar a função de resumo para cada base\n",
    "summary_high = TrainTestSplit.DescriptiveTrainTest(df_train_high, df_test_high, \"High\", outcome_attribute, protected_attribute, privileged_group, unprivileged_group)\n",
    "summary_medium = TrainTestSplit.DescriptiveTrainTest(df_train_medium, df_test_medium, \"Medium\", outcome_attribute, protected_attribute, privileged_group, unprivileged_group)\n",
    "summary_low = TrainTestSplit.DescriptiveTrainTest(df_train_low, df_test_low, \"Low\", outcome_attribute, protected_attribute, privileged_group, unprivileged_group)\n",
    "\n",
    "# Combinar todas as tabelas de resumo em uma única tabela\n",
    "df_summary_table = pd.concat([summary_high, summary_medium, summary_low], axis=1)\n",
    "df_summary_table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77016bf2",
   "metadata": {},
   "source": [
    "<b> Definição das variáveis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fd89752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remover 'Target' da lista de variáveis, caso esteja presente\n",
    "variaveis = [var for var in variaveis if var != 'Target']\n",
    "\n",
    "# Selecionando características e o alvo\n",
    "variaveis_high = [var for var in variaveis if var in df_log_high.columns]\n",
    "X_train_high, y_train_high = df_train_high[variaveis_high], df_train_high['Target']\n",
    "X_test_high, y_test_high = df_test_high[variaveis_high], df_test_high['Target']\n",
    "\n",
    "variaveis_medium = [var for var in variaveis if var in df_log_medium.columns]\n",
    "X_train_medium, y_train_medium = df_train_medium[variaveis_medium], df_train_medium['Target']\n",
    "X_test_medium, y_test_medium = df_test_medium[variaveis_medium], df_test_medium['Target']\n",
    "\n",
    "variaveis_low = [var for var in variaveis if var in df_log_low.columns]\n",
    "X_train_low, y_train_low = df_train_low[variaveis_low], df_train_low['Target']\n",
    "X_test_low, y_test_low = df_test_low[variaveis_low], df_test_low['Target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cad2b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Verificar a existência de colunas que NÃO estão no formato numérico\n",
    "print(\"High:\", X_train_high.select_dtypes(exclude=['number']).columns)\n",
    "print(\"Medium:\", X_train_medium.select_dtypes(exclude=['number']).columns)\n",
    "print(\"Low:\", X_train_low.select_dtypes(exclude=['number']).columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1997a184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar a existência de colunas com valores ausentes (missing values)\n",
    "print(\"High - Missing Values:\", X_train_high.columns[X_train_high.isnull().any()])\n",
    "print(\"Medium - Missing Values:\", X_train_medium.columns[X_train_medium.isnull().any()])\n",
    "print(\"Low - Missing Values:\", X_train_low.columns[X_train_low.isnull().any()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b8f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dimensionalidade - High:\", X_train_high.shape)\n",
    "print(\"Dimensionalidad - Medium:\", X_train_medium.shape)\n",
    "print(\"Dimensionalidad - Low:\", X_train_low.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0c73b4",
   "metadata": {},
   "source": [
    "## Predictive Process Monitoring"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94ec974",
   "metadata": {},
   "source": [
    "### 1. Baseline model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bd5bda",
   "metadata": {},
   "source": [
    "<b> Random Florest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2936f4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso da função:\n",
    "df_final_high, best_model_high, best_params_high, best_score_high, explanations_high = MachineLearning.RandomFlorestOptuna(X_train_high, y_train_high, X_test_high, y_test_high, df_train_high, df_test_high, num_trials=50)\n",
    "df_final_medium, best_model_medium, best_params_medium, best_score_medium, explanations_medium = MachineLearning.RandomFlorestOptuna(X_train_medium, y_train_medium, X_test_medium, y_test_medium, df_train_medium, df_test_medium, num_trials=50)\n",
    "df_final_low, best_model_low, best_params_low, best_score_low, explanations_low = MachineLearning.RandomFlorestOptuna(X_train_low, y_train_low, X_test_low, y_test_low, df_train_low, df_test_low, num_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391acd40",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('High - Parâmetros:', best_params_high)\n",
    "print('Medium - Parâmetros:', best_params_medium)\n",
    "print('Low - Parâmetros:', best_params_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92402b91",
   "metadata": {},
   "source": [
    "Avaliação do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d99045e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Métricas do modelo\n",
    "df_metrics_high = Metrics.ModelMetrics(df_final_high, 'case:protected', 'High', 'Baseline', privileged_group, unprivileged_group)\n",
    "df_metrics_medium = Metrics.ModelMetrics(df_final_medium, 'case:protected', 'Medium', 'Baseline', privileged_group, unprivileged_group)\n",
    "df_metrics_low = Metrics.ModelMetrics(df_final_low, 'case:protected', 'Low', 'Baseline', privileged_group, unprivileged_group)\n",
    "\n",
    "df_metrics = pd.concat([df_metrics_high, df_metrics_medium, df_metrics_low], ignore_index=True)\n",
    "df_metrics.to_excel(f'{name_prefix}_metrics.xlsx', index = False)\n",
    "#df_metrics[df_metrics['Metric'].isin(['Accuracy', 'F1-Score', 'Disparate Impact'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57c3491",
   "metadata": {},
   "source": [
    "Explicação do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e6d221",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_importance(explanations, feature_names):\n",
    "    \"\"\"\n",
    "    Calcula a importância média das variáveis a partir das explicações LIME.\n",
    "    \"\"\"\n",
    "    feature_importances = np.zeros(len(feature_names))\n",
    "    for explanation in explanations:\n",
    "        for feature, importance in explanation.local_exp[1]:  # 1 é a classe alvo\n",
    "            feature_importances[feature] += importance\n",
    "    feature_importances /= len(explanations)\n",
    "    return feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b731e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar tabelas de importância das variáveis\n",
    "importance_high = get_feature_importance(explanations_high, X_train_high.columns)\n",
    "importance_medium = get_feature_importance(explanations_medium, X_train_medium.columns)\n",
    "importance_low = get_feature_importance(explanations_low, X_train_low.columns)\n",
    "\n",
    "# Criar DataFrames com a importância das variáveis\n",
    "importance_high_df = pd.DataFrame({'Feature': X_train_high.columns, 'Importance_High': importance_high})\n",
    "importance_medium_df = pd.DataFrame({'Feature': X_train_medium.columns, 'Importance_Medium': importance_medium})\n",
    "importance_low_df = pd.DataFrame({'Feature': X_train_low.columns, 'Importance_Low': importance_low})\n",
    "\n",
    "# Realizar o merge dos DataFrames utilizando 'Feature' como chave e preenchendo valores faltantes com NaN\n",
    "importance_df = importance_high_df.merge(importance_medium_df, on='Feature', how='outer').merge(importance_low_df, on='Feature', how='outer')\n",
    "\n",
    "# Ordenar o DataFrame com base no módulo dos valores da coluna 'Importance_High'\n",
    "importance_df['Importance_High_Abs'] = importance_df['Importance_High'].abs()\n",
    "importance_df = importance_df.sort_values(by='Importance_High_Abs', ascending=False)\n",
    "importance_df.drop(columns=['Importance_High_Abs'], inplace=True)\n",
    "\n",
    "# Definir o índice como a coluna 'Feature'\n",
    "importance_df.set_index('Feature', inplace=True)\n",
    "\n",
    "# Salvar o DataFrame como CSV\n",
    "importance_df.to_csv(f'{name_prefix}_baseline_model_lime.csv')\n",
    "\n",
    "# Exibir o DataFrame\n",
    "importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771f8d73",
   "metadata": {},
   "source": [
    "### 2. Estratégia para Fairness"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee91b60d",
   "metadata": {},
   "source": [
    "<b> 2.1 Reweighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710da85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso da função:\n",
    "df_final_high, best_model_high, best_params_high, best_score_high, explanations_high = AIF360.PreReweighingRandomFlorestOptuna(X_train_high, y_train_high, X_test_high, y_test_high, df_train_high, df_test_high, protected_attribute, alpha = 0.1, num_trials=50)\n",
    "df_final_medium, best_model_medium, best_params_medium, best_score_medium, explanations_medium = AIF360.PreReweighingRandomFlorestOptuna(X_train_medium, y_train_medium, X_test_medium, y_test_medium, df_train_medium, df_test_medium, protected_attribute, alpha = 0.1, num_trials=50)\n",
    "df_final_low, best_model_low, best_params_low, best_score_low, explanations_low = AIF360.PreReweighingRandomFlorestOptuna(X_train_low, y_train_low, X_test_low, y_test_low, df_train_low, df_test_low, protected_attribute, alpha = 0.1, num_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e784f3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('High - Parâmetros:', best_params_high)\n",
    "print('Medium - Parâmetros:', best_params_medium)\n",
    "print('Low - Parâmetros:', best_params_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf85e79",
   "metadata": {},
   "source": [
    "Avaliação do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9797d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Métricas do modelo\n",
    "df_metrics_high = Metrics.ModelMetrics(df_final_high, 'case:protected', 'High', 'Preprocessing: Reweighing', privileged_group, unprivileged_group)\n",
    "df_metrics_medium = Metrics.ModelMetrics(df_final_medium, 'case:protected', 'Medium', 'Preprocessing: Reweighing', privileged_group, unprivileged_group)\n",
    "df_metrics_low = Metrics.ModelMetrics(df_final_low, 'case:protected', 'Low', 'Preprocessing: Reweighing', privileged_group, unprivileged_group)\n",
    "\n",
    "df_metrics_Rew = pd.concat([df_metrics_high, df_metrics_medium, df_metrics_low], ignore_index=True)\n",
    "df_metrics = pd.concat([df_metrics, df_metrics_Rew], ignore_index=True)\n",
    "df_metrics.to_excel(f'{name_prefix}_metrics.xlsx', index = False)\n",
    "#df_metrics[df_metrics['Metric'].isin(['Accuracy', 'F1-Score', 'Disparate Impact'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccc4fdf",
   "metadata": {},
   "source": [
    "Explicação do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1bdcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar tabelas de importância das variáveis\n",
    "importance_high = get_feature_importance(explanations_high, X_train_high.columns)\n",
    "importance_medium = get_feature_importance(explanations_medium, X_train_medium.columns)\n",
    "importance_low = get_feature_importance(explanations_low, X_train_low.columns)\n",
    "\n",
    "# Criar DataFrames com a importância das variáveis\n",
    "importance_high_df = pd.DataFrame({'Feature': X_train_high.columns, 'Importance_High': importance_high})\n",
    "importance_medium_df = pd.DataFrame({'Feature': X_train_medium.columns, 'Importance_Medium': importance_medium})\n",
    "importance_low_df = pd.DataFrame({'Feature': X_train_low.columns, 'Importance_Low': importance_low})\n",
    "\n",
    "# Realizar o merge dos DataFrames utilizando 'Feature' como chave e preenchendo valores faltantes com NaN\n",
    "importance_df = importance_high_df.merge(importance_medium_df, on='Feature', how='outer').merge(importance_low_df, on='Feature', how='outer')\n",
    "\n",
    "# Ordenar o DataFrame com base no módulo dos valores da coluna 'Importance_High'\n",
    "importance_df['Importance_High_Abs'] = importance_df['Importance_High'].abs()\n",
    "importance_df = importance_df.sort_values(by='Importance_High_Abs', ascending=False)\n",
    "importance_df.drop(columns=['Importance_High_Abs'], inplace=True)\n",
    "\n",
    "# Definir o índice como a coluna 'Feature'\n",
    "importance_df.set_index('Feature', inplace=True)\n",
    "\n",
    "# Salvar o DataFrame como CSV\n",
    "importance_df.to_csv(f'{name_prefix}_pre_rew_model_lime.csv')\n",
    "\n",
    "# Exibir o DataFrame\n",
    "importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e82ca0d",
   "metadata": {},
   "source": [
    "<b> 2.2 Disparate Impact Remover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a08dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso da função:\n",
    "df_final_high, best_model_high, best_params_high, best_score_high, explanations_high = AIF360.PreDisparateImpactRemoverRandomFlorestOptuna(X_train_high, y_train_high, X_test_high, y_test_high, df_train_high, df_test_high, protected_attribute, alpha = 0.1, num_trials=50)\n",
    "df_final_medium, best_model_medium, best_params_medium, best_score_medium, explanations_medium = AIF360.PreDisparateImpactRemoverRandomFlorestOptuna(X_train_medium, y_train_medium, X_test_medium, y_test_medium, df_train_medium, df_test_medium, protected_attribute, alpha = 0.1, num_trials=50)\n",
    "df_final_low, best_model_low, best_params_low, best_score_low, explanations_low = AIF360.PreDisparateImpactRemoverRandomFlorestOptuna(X_train_low, y_train_low, X_test_low, y_test_low, df_train_low, df_test_low, protected_attribute, alpha = 0.1, num_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706eb938",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('High - Parâmetros:', best_params_high)\n",
    "print('Medium - Parâmetros:', best_params_medium)\n",
    "print('Low - Parâmetros:', best_params_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5da54e5",
   "metadata": {},
   "source": [
    "Avaliação do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9192ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Métricas do modelo\n",
    "df_metrics_high = Metrics.ModelMetrics(df_final_high, 'case:protected', 'High', 'Preprocessing: Disparate Impact Remover', privileged_group, unprivileged_group)\n",
    "df_metrics_medium = Metrics.ModelMetrics(df_final_medium, 'case:protected', 'Medium', 'Preprocessing: Disparate Impact Remover', privileged_group, unprivileged_group)\n",
    "df_metrics_low = Metrics.ModelMetrics(df_final_low, 'case:protected', 'Low', 'Preprocessing: Disparate Impact Remover', privileged_group, unprivileged_group)\n",
    "\n",
    "df_metrics_Dir = pd.concat([df_metrics_high, df_metrics_medium, df_metrics_low], ignore_index=True)\n",
    "df_metrics = pd.concat([df_metrics, df_metrics_Dir], ignore_index=True)\n",
    "df_metrics.to_excel(f'{name_prefix}_metrics.xlsx', index = False)\n",
    "#df_metrics[df_metrics['Metric'].isin(['Accuracy', 'F1-Score', 'Disparate Impact'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28e30f8",
   "metadata": {},
   "source": [
    "Explicação do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65161d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar tabelas de importância das variáveis\n",
    "importance_high = get_feature_importance(explanations_high, X_train_high.columns)\n",
    "importance_medium = get_feature_importance(explanations_medium, X_train_medium.columns)\n",
    "importance_low = get_feature_importance(explanations_low, X_train_low.columns)\n",
    "\n",
    "# Criar DataFrames com a importância das variáveis\n",
    "importance_high_df = pd.DataFrame({'Feature': X_train_high.columns, 'Importance_High': importance_high})\n",
    "importance_medium_df = pd.DataFrame({'Feature': X_train_medium.columns, 'Importance_Medium': importance_medium})\n",
    "importance_low_df = pd.DataFrame({'Feature': X_train_low.columns, 'Importance_Low': importance_low})\n",
    "\n",
    "# Realizar o merge dos DataFrames utilizando 'Feature' como chave e preenchendo valores faltantes com NaN\n",
    "importance_df = importance_high_df.merge(importance_medium_df, on='Feature', how='outer').merge(importance_low_df, on='Feature', how='outer')\n",
    "\n",
    "# Ordenar o DataFrame com base no módulo dos valores da coluna 'Importance_High'\n",
    "importance_df['Importance_High_Abs'] = importance_df['Importance_High'].abs()\n",
    "importance_df = importance_df.sort_values(by='Importance_High_Abs', ascending=False)\n",
    "importance_df.drop(columns=['Importance_High_Abs'], inplace=True)\n",
    "\n",
    "# Definir o índice como a coluna 'Feature'\n",
    "importance_df.set_index('Feature', inplace=True)\n",
    "\n",
    "# Salvar o DataFrame como CSV\n",
    "importance_df.to_csv(f'{name_prefix}_pre_dir_model_lime.csv')\n",
    "\n",
    "# Exibir o DataFrame\n",
    "importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d78aa8",
   "metadata": {},
   "source": [
    "<b> 2.3 Adversarial Debiasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5fa6179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso da função:\n",
    "df_final_high, best_model_high, best_params_high, best_score_high, explanations_high = AIF360.InAdversarialDebiasingOptuna(X_train_high, y_train_high, X_test_high, y_test_high, df_train_high, df_test_high, protected_attribute, alpha = 0.1, num_trials=50)\n",
    "df_final_medium, best_model_medium, best_params_medium, best_score_medium, explanations_medium = AIF360.InAdversarialDebiasingOptuna(X_train_medium, y_train_medium, X_test_medium, y_test_medium, df_train_medium, df_test_medium, protected_attribute, alpha = 0.1, num_trials=50)\n",
    "df_final_low, best_model_low, best_params_low, best_score_low, explanations_low = AIF360.InAdversarialDebiasingOptuna(X_train_low, y_train_low, X_test_low, y_test_low, df_train_low, df_test_low, protected_attribute, alpha = 0.1, num_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8543a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('High - Parâmetros:', best_params_high)\n",
    "print('Medium - Parâmetros:', best_params_medium)\n",
    "print('Low - Parâmetros:', best_params_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93db46c0",
   "metadata": {},
   "source": [
    "Avaliação do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60f929e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Métricas do modelo\n",
    "df_metrics_high = Metrics.ModelMetrics(df_final_high, 'case:protected', 'High', 'Inprocessing: Adversarial Debiasing', privileged_group, unprivileged_group)\n",
    "df_metrics_medium = Metrics.ModelMetrics(df_final_medium, 'case:protected', 'Medium', 'Inprocessing: Adversarial Debiasing', privileged_group, unprivileged_group)\n",
    "df_metrics_low = Metrics.ModelMetrics(df_final_low, 'case:protected', 'Low', 'Inprocessing: Adversarial Debiasing', privileged_group, unprivileged_group)\n",
    "\n",
    "df_metrics_Adv = pd.concat([df_metrics_high, df_metrics_medium, df_metrics_low], ignore_index=True)\n",
    "df_metrics = pd.concat([df_metrics, df_metrics_Adv], ignore_index=True)\n",
    "df_metrics.to_excel(f'{name_prefix}_metrics.xlsx', index = False)\n",
    "#df_metrics[df_metrics['Metric'].isin(['Accuracy', 'F1-Score', 'Disparate Impact'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba25896",
   "metadata": {},
   "source": [
    "Explicação do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337ab8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar tabelas de importância das variáveis\n",
    "importance_high = get_feature_importance(explanations_high, X_train_high.columns)\n",
    "importance_medium = get_feature_importance(explanations_medium, X_train_medium.columns)\n",
    "importance_low = get_feature_importance(explanations_low, X_train_low.columns)\n",
    "\n",
    "# Criar DataFrames com a importância das variáveis\n",
    "importance_high_df = pd.DataFrame({'Feature': X_train_high.columns, 'Importance_High': importance_high})\n",
    "importance_medium_df = pd.DataFrame({'Feature': X_train_medium.columns, 'Importance_Medium': importance_medium})\n",
    "importance_low_df = pd.DataFrame({'Feature': X_train_low.columns, 'Importance_Low': importance_low})\n",
    "\n",
    "# Realizar o merge dos DataFrames utilizando 'Feature' como chave e preenchendo valores faltantes com NaN\n",
    "importance_df = importance_high_df.merge(importance_medium_df, on='Feature', how='outer').merge(importance_low_df, on='Feature', how='outer')\n",
    "\n",
    "# Ordenar o DataFrame com base no módulo dos valores da coluna 'Importance_High'\n",
    "importance_df['Importance_High_Abs'] = importance_df['Importance_High'].abs()\n",
    "importance_df = importance_df.sort_values(by='Importance_High_Abs', ascending=False)\n",
    "importance_df.drop(columns=['Importance_High_Abs'], inplace=True)\n",
    "\n",
    "# Definir o índice como a coluna 'Feature'\n",
    "importance_df.set_index('Feature', inplace=True)\n",
    "\n",
    "# Salvar o DataFrame como CSV\n",
    "importance_df.to_csv(f'{name_prefix}_in_adv_model_lime.csv')\n",
    "\n",
    "# Exibir o DataFrame\n",
    "importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9679fa",
   "metadata": {},
   "source": [
    "<b> 2.4 Equalized Odds Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37401057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso da função:\n",
    "#df_final_high, best_model_high, best_params_high, best_score_high, explanations_high = AIF360.PostEOddsPostprocessingRandomFlorestOptuna(X_train_high, y_train_high, X_test_high, y_test_high, df_train_high, df_test_high, protected_attribute, alpha = 0.1, num_trials=50)\n",
    "#df_final_medium, best_model_medium, best_params_medium, best_score_medium, explanations_medium = AIF360.PostEOddsPostprocessingRandomFlorestOptuna(X_train_medium, y_train_medium, X_test_medium, y_test_medium, df_train_medium, df_test_medium, protected_attribute, alpha = 0.1, num_trials=50)\n",
    "#df_final_low, best_model_low, best_params_low, best_score_low, explanations_low = AIF360.PostEOddsPostprocessingRandomFlorestOptuna(X_train_low, y_train_low, X_test_low, y_test_low, df_train_low, df_test_low, protected_attribute, alpha = 0.1, num_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73a03cf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Serão os parâmetros da Random Florest\n",
    "#print('High - Parâmetros:', best_params_high)\n",
    "#print('Medium - Parâmetros:', best_params_medium)\n",
    "#print('Low - Parâmetros:', best_params_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62932111",
   "metadata": {},
   "source": [
    "Avaliação do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a07c4c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Métricas do modelo\n",
    "#df_metrics_high = Metrics.ModelMetrics(df_final_high, 'case:protected', 'High', 'Postprocessing: Equalized Odds', privileged_group, unprivileged_group)\n",
    "#df_metrics_medium = Metrics.ModelMetrics(df_final_medium, 'case:protected', 'Medium', 'Postprocessing: Equalized Odds', privileged_group, unprivileged_group)\n",
    "#df_metrics_low = Metrics.ModelMetrics(df_final_low, 'case:protected', 'Low', 'Postprocessing: Equalized Odds', privileged_group, unprivileged_group)\n",
    "\n",
    "#df_metrics_PostEOdds = pd.concat([df_metrics_high, df_metrics_medium, df_metrics_low], ignore_index=True)\n",
    "#df_metrics = pd.concat([df_metrics, df_metrics_PostEOdds], ignore_index=True)\n",
    "#df_metrics.to_excel(f'{name_prefix}_metrics.xlsx', index = False)\n",
    "#df_metrics[df_metrics['Metric'].isin(['Accuracy', 'F1-Score', 'Disparate Impact'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044498cc",
   "metadata": {},
   "source": [
    "Explicação do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e324cf88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar tabelas de importância das variáveis\n",
    "#importance_high = get_feature_importance(explanations_high, X_train_high.columns)\n",
    "#importance_medium = get_feature_importance(explanations_medium, X_train_medium.columns)\n",
    "#importance_low = get_feature_importance(explanations_low, X_train_low.columns)\n",
    "\n",
    "# Criar DataFrames com a importância das variáveis\n",
    "#importance_high_df = pd.DataFrame({'Feature': X_train_high.columns, 'Importance_High': importance_high})\n",
    "#importance_medium_df = pd.DataFrame({'Feature': X_train_medium.columns, 'Importance_Medium': importance_medium})\n",
    "#importance_low_df = pd.DataFrame({'Feature': X_train_low.columns, 'Importance_Low': importance_low})\n",
    "\n",
    "# Realizar o merge dos DataFrames utilizando 'Feature' como chave e preenchendo valores faltantes com NaN\n",
    "#importance_df = importance_high_df.merge(importance_medium_df, on='Feature', how='outer').merge(importance_low_df, on='Feature', how='outer')\n",
    "\n",
    "# Ordenar o DataFrame com base no módulo dos valores da coluna 'Importance_High'\n",
    "#importance_df['Importance_High_Abs'] = importance_df['Importance_High'].abs()\n",
    "#importance_df = importance_df.sort_values(by='Importance_High_Abs', ascending=False)\n",
    "#importance_df.drop(columns=['Importance_High_Abs'], inplace=True)\n",
    "\n",
    "# Definir o índice como a coluna 'Feature'\n",
    "#importance_df.set_index('Feature', inplace=True)\n",
    "\n",
    "# Salvar o DataFrame como CSV\n",
    "#importance_df.to_csv(f'{name_prefix}_post_eodds_model_lime.csv')\n",
    "\n",
    "# Exibir o DataFrame\n",
    "#importance_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3792ef10",
   "metadata": {},
   "source": [
    "<b> 2.5 Calibrated EqOdds Postprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbe8e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uso da função:\n",
    "df_final_high, best_model_high, best_params_high, best_score_high, explanations_high = AIF360.PostCalibratedEOddsRandomFlorestOptuna(X_train_high, y_train_high, X_test_high, y_test_high, df_train_high, df_test_high, protected_attribute, alpha = 0.1, num_trials=50)\n",
    "df_final_medium, best_model_medium, best_params_medium, best_score_medium, explanations_medium = AIF360.PostCalibratedEOddsRandomFlorestOptuna(X_train_medium, y_train_medium, X_test_medium, y_test_medium, df_train_medium, df_test_medium, protected_attribute, alpha = 0.1, num_trials=50)\n",
    "df_final_low, best_model_low, best_params_low, best_score_low, explanations_low = AIF360.PostCalibratedEOddsRandomFlorestOptuna(X_train_low, y_train_low, X_test_low, y_test_low, df_train_low, df_test_low, protected_attribute, alpha = 0.1, num_trials=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0facb2c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('High - Parâmetros:', best_params_high)\n",
    "print('Medium - Parâmetros:', best_params_medium)\n",
    "print('Low - Parâmetros:', best_params_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037510d0",
   "metadata": {},
   "source": [
    "Avaliação do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf4b626",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Métricas do modelo\n",
    "df_metrics_high = Metrics.ModelMetrics(df_final_high, 'case:protected', 'High', 'Postprocessing: Calibrated EqOdds', privileged_group, unprivileged_group)\n",
    "df_metrics_medium = Metrics.ModelMetrics(df_final_medium, 'case:protected', 'Medium', 'Postprocessing: Calibrated EqOdds', privileged_group, unprivileged_group)\n",
    "df_metrics_low = Metrics.ModelMetrics(df_final_low, 'case:protected', 'Low', 'Postprocessing: Calibrated EqOdds', privileged_group, unprivileged_group)\n",
    "\n",
    "df_metrics_PostCalibrated = pd.concat([df_metrics_high, df_metrics_medium, df_metrics_low], ignore_index=True)\n",
    "df_metrics = pd.concat([df_metrics, df_metrics_PostCalibrated], ignore_index=True)\n",
    "df_metrics.to_excel(f'{name_prefix}_metrics.xlsx', index = False)\n",
    "#df_metrics[df_metrics['Metric'].isin(['Accuracy', 'F1-Score', 'Disparate Impact'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200898e4",
   "metadata": {},
   "source": [
    "Explicação do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69d240e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerar tabelas de importância das variáveis\n",
    "importance_high = get_feature_importance(explanations_high, X_train_high.columns)\n",
    "importance_medium = get_feature_importance(explanations_medium, X_train_medium.columns)\n",
    "importance_low = get_feature_importance(explanations_low, X_train_low.columns)\n",
    "\n",
    "# Criar DataFrames com a importância das variáveis\n",
    "importance_high_df = pd.DataFrame({'Feature': X_train_high.columns, 'Importance_High': importance_high})\n",
    "importance_medium_df = pd.DataFrame({'Feature': X_train_medium.columns, 'Importance_Medium': importance_medium})\n",
    "importance_low_df = pd.DataFrame({'Feature': X_train_low.columns, 'Importance_Low': importance_low})\n",
    "\n",
    "# Realizar o merge dos DataFrames utilizando 'Feature' como chave e preenchendo valores faltantes com NaN\n",
    "importance_df = importance_high_df.merge(importance_medium_df, on='Feature', how='outer').merge(importance_low_df, on='Feature', how='outer')\n",
    "\n",
    "# Ordenar o DataFrame com base no módulo dos valores da coluna 'Importance_High'\n",
    "importance_df['Importance_High_Abs'] = importance_df['Importance_High'].abs()\n",
    "importance_df = importance_df.sort_values(by='Importance_High_Abs', ascending=False)\n",
    "importance_df.drop(columns=['Importance_High_Abs'], inplace=True)\n",
    "\n",
    "# Definir o índice como a coluna 'Feature'\n",
    "importance_df.set_index('Feature', inplace=True)\n",
    "\n",
    "# Salvar o DataFrame como CSV\n",
    "importance_df.to_csv(f'{name_prefix}_post_calibrated_model_lime.csv')\n",
    "\n",
    "# Exibir o DataFrame\n",
    "importance_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
